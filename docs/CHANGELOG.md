# Changelog

All notable changes to Polarway will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.0] - 2026-02-26

### üéâ First Stable Release

Polarway v1.0.0 establishes the first stable, production-ready API for the Railway-Oriented DataFrame engine.

### ‚ú® New in 1.0.0

#### Lakehouse (Delta Lake support)
- ACID transactions with time-travel queries (`read_version`, `read_timestamp`)
- User management with Argon2 password hashing and JWT authentication
- Role-based access control (RBAC) for table-level permissions
- Audit logging for compliance (configurable retention)
- Delta Lake maintenance operations (vacuum, optimize, z-order clustering)
- DataFusion SQL engine integration for complex analytics queries

#### Python Package (`polarway` on PyPI)
- gRPC client library: `from polarway import PolarwayClient, TimeSeriesClient`
- `LakehouseClient` for Delta Lake operations with optional dependencies
- Async iterator streaming API: `async for batch in client.stream_query(...)`
- Arrow IPC zero-copy deserialization
- Automatic proto stub generation at install time
- Full type annotations throughout

#### Functional Programming Enhancements
- Railway-Oriented Programming (ROP) monad: `Result<T, E>`, `Option<T>`
- `Maybe` monad for optional chaining
- `pipe()`, `compose()`, `curry()` higher-order utilities
- Lens-based immutable update patterns
- `validate_dataframe()` with typed schema contracts

#### Notebooks (verified, all cells pass)
- `environment_check.ipynb` ‚Äî dependency verification (‚úÖ pass)
- `polarway_showcase.ipynb` ‚Äî full feature tour (‚úÖ pass)
- `polarway_functional_programming.ipynb` ‚Äî ROP / monad patterns (‚úÖ pass)
- `polarway_functional_programming_v2.ipynb` ‚Äî advanced composition patterns (‚úÖ pass)
- `storage_demo.ipynb` ‚Äî LRU cache + Parquet + DuckDB hot/cold storage (‚úÖ pass)
- `polarway_advanced.ipynb` ‚Äî streaming joins, ETL, partitioning (‚úÖ pass)
- `polarway_cloud_integrations.ipynb` ‚Äî CSV/JSON/Parquet + SQLite + Pandas (‚úÖ pass)
- `adaptive_streaming_benchmarks.ipynb` ‚Äî requires private `polars_streaming_adaptive` build (‚ö†Ô∏è internal only)
- Notebooks requiring a running Polarway gRPC server (‚ö†Ô∏è server required):
  `phase2_operations_test.ipynb`, `phase5_streaming_test.ipynb`, `rest_exec_api_demo.ipynb`

#### Documentation
- Switched ReadTheDocs build to MkDocs Material
- Cleaned `mkdocs.yml`: removed Polars-upstream nav (polars-cloud, polars-on-premises)
- Fixed repo URL to `ThotDjehuty/polarway`
- Added 25 reference docs to nav

### üîß Changed
- `polarway-grpc` crate: version bumped `0.1.0` ‚Üí `1.0.0`
- `polarway` Python package: version bumped `0.1.1` ‚Üí `1.0.0`
- Package description updated to reflect production-ready status
- Development Status classifier: Alpha ‚Üí Beta
- `docs/source/conf.py`: release `0.53.0` ‚Üí `1.0.0`

### ‚úÖ Test Results
- Python tests: **27/27 pass** (`pytest polarway-python/tests/`)
- Dependencies: `deltalake==1.4.2`, `pyarrow==18.1.0`, `argon2-cffi`, `PyJWT` installed in rhftlab
- Standalone notebooks: 7 notebooks pass `nbconvert --execute --inplace`
- Rust compilation: known `arrow-arith@53.4.0` √ó `chrono@0.4.39+` conflict (see ‚ö†Ô∏è Known Issues)

### ‚ö†Ô∏è Known Issues
- **Rust build conflict**: `arrow-arith 53.4.0` introduced a `quarter()` ambiguity with `chrono >= 0.4.39` (chrono added `Datelike::quarter()` which conflicts with `ChronoDateExt::quarter()`). Fix: upgrade arrow to 54+ or use a workspace-level `[patch.crates-io]`. This affects compilation but not the Python package (which uses grpcio, not the Rust gRPC binary).
- **crates.io publication deferred**: `polarway-grpc` uses path dependencies on local Polars fork crates (`path = "../crates/polars"`). crates.io requires all dependencies to come from crates.io. Architecture change required in v1.1.0.
- **pyarrow compatibility**: `pyarrow >= 21` has a binary incompatibility with `_azurefs` when `deltalake` is installed. Use `pyarrow == 18.1.0` for the Lakehouse optional dependency.

### üîó Links
- PyPI: https://pypi.org/project/polarway/ (release pending)
- crates.io: deferred to v1.1.0 (path dependency architecture change required)
- ReadTheDocs: https://polarway.readthedocs.io
- Repository: https://github.com/ThotDjehuty/polarway

---

## [0.1.1] - 2026-02-11

### üêõ Bug Fixes

#### Rust Compilation
- **PyO3 Deprecations**: Updated all 26 instances of deprecated `PyObject` to `Py<PyAny>` in polars-python bindings
- **Tracing Macros**: Fixed error logging format from Display (`%e`) to Debug (`?e`) for custom error types in lakehouse, maintenance, auth, and audit modules
- **TableProvider Integration**: Fixed trait object creation for DataFusion `TableProvider` in 5 locations (scan, query, sql, read_version, read_timestamp methods)
- **Code Warnings**: Suppressed false-positive dead code warnings in streaming-adaptive `mmap_reader` and timeseries `vwap` modules
- **Syntax**: Removed redundant trailing semicolons

#### Python Type Checking
- **Pylance Configuration**: Added `pyrightconfig.json` with appropriate settings for optional dependencies
- **Type Safety**: Fixed type mismatch in `approve_user` return value with explicit None check
- **Import Warnings**: Reduced Pylance errors from 36 to 13 by adding file-level type checking directives

### ‚úÖ Testing
- All 8 unit tests pass successfully
- Zero compilation errors with `cargo clippy --all-targets`
- Only 3 style warnings remaining (FromStr trait suggestions)
- Compilation time: ~6.2s

### üìù Notes
- Remaining Python type errors are false positives for optional dependencies (deltalake, argon2-cffi, PyJWT)
- These dependencies are runtime-checked in `LakehouseClient.__init__`
- Remaining rust-analyzer LSP errors are false positives (cargo compiles successfully)

## [0.1.0] - 2026-01-12

### üéâ Initial Public Release

First public release of Polarway - a high-performance DataFrame library with gRPC streaming architecture.

### ‚ú® Features

#### Core Operations
- Read/write Parquet, CSV, JSON with schema inference
- Select, filter, group_by, join, sort, aggregate operations
- Lazy evaluation with automatic query optimization
- Predicate and projection pushdown to data sources
- Zero-copy Apache Arrow serialization
- Handle-based architecture for memory-efficient operations

#### Time-Series Support
- `TimeSeriesFrame` with frequency-aware operations
- OHLCV resampling (tick ‚Üí 1m ‚Üí 5m ‚Üí 1h ‚Üí 1d)
- Rolling window aggregations (SMA, EMA, Bollinger bands)
- Lag, lead, diff, and pct_change operations
- As-of joins for time-aligned data

#### Streaming & Network Sources
- WebSocket streaming with automatic reconnection
- REST API pagination (cursor-based, offset, link headers)
- Kafka, NATS, Redis Streams integration
- Real-time data pipelines with backpressure handling
- Async Tokio runtime for concurrent operations

#### gRPC Server
- Tonic-based gRPC server on port 50051
- Handle lifecycle management with configurable TTL
- Arrow IPC streaming for efficient data transfer
- Rust-based monadic error handling (Result<T, E>)

### üìö Documentation
- Comprehensive README with Quick Start guide
- Docker deployment instructions
- Practical examples (time-series, WebSocket, concurrent operations)
- Clear Contributing guidelines
- Performance benchmarks vs Polars

### üèóÔ∏è Architecture
- Client-server architecture with handle-based operations
- DataFrames stay on server, clients hold references
- Language-agnostic gRPC API
- Immutable operations (functional programming principles)
- Built on Polars core with gRPC interface layer

### üôè Acknowledgments
- Forked from [Polars](https://github.com/pola-rs/polars) v0.52.0
- Built with Apache Arrow, DataFusion, Tonic, and Tokio

---

[0.1.0]: https://github.com/EnkiNudimmud/polarway/releases/tag/v0.1.0
